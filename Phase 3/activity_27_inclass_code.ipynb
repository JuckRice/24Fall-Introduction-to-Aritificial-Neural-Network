{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yIf63ka8W2MO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "  def __init__(self, num_classes=10):\n",
    "    super(LeNet5, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 6, kernel_size=5) # 1 input channel, 6 output channels\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "    self.relu2 = nn.ReLU()\n",
    "    self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    self.fc1 = nn.Linear(16*4*4, 120)\n",
    "    self.relu3 = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    self.relu4 = nn.ReLU()\n",
    "    self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.avgpool1(self.relu1(self.conv1(x)))\n",
    "    x = self.avgpool2(self.relu2(self.conv2(x)))\n",
    "    x = x.view(-1, 16*4*4)\n",
    "    x = self.relu3(self.fc1(x))\n",
    "    x = self.relu4(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CcEToUuW9_X",
    "outputId": "ab9f3a5a-1ad0-4628-9a02-34e2f4ac51b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 31970372.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 1198591.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 10679857.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307589\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.210147\n",
      "Accuracy of the network on the 10000 test images: 96.89 %\n"
     ]
    }
   ],
   "source": [
    "# prompt: train LeNet5 with MNIST\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Instantiate the model and move it to the device\n",
    "model = LeNet5().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_idx % 500 == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch + 1, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "# Testing loop\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "  for data, target in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model(data)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    total += target.size(0)\n",
    "    correct += (predicted == target).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKuh01LXXOUm",
    "outputId": "8c71ab60-b56a-4a82-f2c7-7c629a9d1233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:02<00:00, 12034030.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 256410.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:00<00:00, 4870802.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Accuracy of the network on the 10000 test images: 8.14 %\n"
     ]
    }
   ],
   "source": [
    "# prompt: load FMNIST data, and evaluate LeNET5's accuracy as it is\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the FashionMNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST('./data', train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# ... (rest of your LeNet5 class definition from the previous response)\n",
    "\n",
    "# Instantiate the model and move it to the device\n",
    "model = LeNet5(num_classes=10).to(device) # Assuming 10 classes for FashionMNIST\n",
    "\n",
    "# ... (rest of your code for defining loss, optimizer, training loop, and testing loop,\n",
    "#       but make sure to use the FashionMNIST dataloaders)\n",
    "# Testing loop\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiYF4QZtXWrf",
    "outputId": "c59085d4-8379-4191-fa29-b6f18fa8f2f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309640\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.251570\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.230162\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.190902\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.136180\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.081304\n"
     ]
    }
   ],
   "source": [
    "# prompt: let's freeze everything but the last fc3 layer.  finetuen fc3 layer for one epoch with FMNIST\n",
    "\n",
    "# Freeze all layers except fc3\n",
    "for name, param in model.named_parameters():\n",
    "    if name not in ['fc3.weight', 'fc3.bias']:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define the optimizer for only the fc3 layer\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# Training loop (only for one epoch)\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52dfjjhLXfz0",
    "outputId": "978be669-3f7d-4a3c-d939-56abdd6a52bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 47.2 %\n"
     ]
    }
   ],
   "source": [
    "# prompt: show eval on fMNIST\n",
    "\n",
    "# Testing loop\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NvvJronEXjrE",
    "outputId": "6c5d52a6-aae9-47f6-bb0f-4acdc600ce3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.080589\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.584328\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.475998\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.374677\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.425718\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.277348\n"
     ]
    }
   ],
   "source": [
    "# prompt: let's freeze everything but the last fc3 layer.  finetuen fc3 layer for one epoch with FMNIST\n",
    "\n",
    "# Freeze all layers except fc3\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Define the optimizer for only the fc3 layer\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# Training loop (only for one epoch)\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HpEOXCHcKii",
    "outputId": "ccd38f1b-2087-4f8a-d302-c8167b31987a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 87.24 %\n"
     ]
    }
   ],
   "source": [
    "# prompt: show eval on fMNIST\n",
    "\n",
    "# Testing loop\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "das1IbmKX33F"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import Required\n",
    "# prompt: let's try Lora. first define a new model class where in the last FC layers, we have lora parameters. then, copy the pretrained model parameters from \"model\". then train lora parameters while freezing everything else\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, r, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Linear(in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, out_features, bias=False)\n",
    "        self.scaling = 1/r\n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lora_B(self.lora_A(x)) * self.scaling\n",
    "\n",
    "\n",
    "class LeNet5_Lora(nn.Module):\n",
    "    def __init__(self, num_classes=10, r=4):\n",
    "        super(LeNet5_Lora, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5) # 1 input channel, 6 output channels\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "        # Lora\n",
    "        self.lora_fc1 = LoRALayer(r, 16*4*4, 120)\n",
    "        self.lora_fc2 = LoRALayer(r, 120, 84)\n",
    "        self.lora_fc3 = LoRALayer(r, 84, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool1(self.relu1(self.conv1(x)))\n",
    "        x = self.avgpool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = self.relu3(self.fc1(x) + self.lora_fc1(x))\n",
    "        x = self.relu4(self.fc2(x) + self.lora_fc2(x))\n",
    "        x = self.fc3(x) + self.lora_fc3(x)\n",
    "        return x\n",
    "\n",
    "# Assuming 'model' is your pre-trained LeNet5 model\n",
    "lora_model = LeNet5_Lora().to(device)\n",
    "\n",
    "#copy weights\n",
    "lora_model.conv1.weight.data = model.conv1.weight.data\n",
    "lora_model.conv1.bias.data = model.conv1.bias.data\n",
    "lora_model.conv2.weight.data = model.conv2.weight.data\n",
    "lora_model.conv2.bias.data = model.conv2.bias.data\n",
    "lora_model.fc1.weight.data = model.fc1.weight.data\n",
    "lora_model.fc1.bias.data = model.fc1.bias.data\n",
    "lora_model.fc2.weight.data = model.fc2.weight.data\n",
    "lora_model.fc2.bias.data = model.fc2.bias.data\n",
    "lora_model.fc3.weight.data = model.fc3.weight.data\n",
    "lora_model.fc3.bias.data = model.fc3.bias.data\n",
    "\n",
    "# Freeze all layers except lora parameters\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define optimizer for only lora parameters\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, lora_model.parameters()), lr=0.001)\n",
    "\n",
    "# Training loop (example)\n",
    "# ... (your training loop using lora_model and optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_li5JNeYqTc",
    "outputId": "0da37149-a53b-4da4-ef01-3819546c3e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.267949\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.167649\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.345691\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.305637\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.323810\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.340919\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = lora_model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
